{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, dill, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse \n",
    "from sklearn.linear_model import LogisticRegressionCV, PassiveAggressiveClassifier, SGDClassifier, Perceptron\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "from sklearn.decomposition import SparsePCA, TruncatedSVD, IncrementalPCA\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.utils import shuffle\n",
    "import seaborn as sns\n",
    "from pprint import pprint\n",
    "\n",
    "import mlflow, mlflow.sklearn\n",
    "import warnings\n",
    "\n",
    "# for importing from sibling directories\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../02_output'))\n",
    "sys.path.append(os.path.abspath('../03_data'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Load numpy npy and featurename files, data integrity checks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "PitlerNenkova_Conn train\n",
      "Lin_etal train\n",
      "Li_etal16 train\n",
      "PitlerNenkova_Conn dev\n",
      "Lin_etal dev\n",
      "Li_etal16 dev\n",
      "PitlerNenkova_Conn test\n",
      "Lin_etal test\n",
      "Li_etal16 test\n"
     ]
    }
   ],
   "source": [
    "##### Environment variables #####\n",
    "var_dict = {}\n",
    "LANG = 'en'\n",
    "RUN = 'UD1_Gold_Run2' \n",
    "# run names are: (i) PTB_Gold_Run1, (ii) PTB_Auto_Run1, (iii) PTB_Gold_Run2, (iv) PTB_Auto_Run2, (v) UD1_Gold_Run2\n",
    "# (vi) UD1_Auto_Run2\n",
    "\n",
    "##### Mapping featuresets to Runs \n",
    "# folder name to variable name mappings \n",
    "feat_var_mapping = {'PitlerNenkova_Conn': 'PNconn', 'PitlerNenkova_Syn': 'PNsyn', \n",
    "                    'PitlerNenkova_ConnSyn':'PNconnsyn', 'PitlerNenkova_SynSyn':'PNsynsyn','Lin_etal': 'Lin', \n",
    "                    'Li_etal16': 'Li16'}\n",
    "if 'Run2' in RUN:\n",
    "    [feat_var_mapping.pop(i) for i in ['PitlerNenkova_ConnSyn','PitlerNenkova_SynSyn', 'PitlerNenkova_Syn']]\n",
    "\n",
    "X_filepath = '../02_output/{}/{}/{}/{}_{}.npz' # LANG, RUN, feat_class, dataset\n",
    "y_filepath = '../02_output/{}/{}/{}/{}_{}.npy' # LANG, RUN, feat_class, dataset\n",
    "featname_filepath = '../02_output/{}/{}/{}/{}_featnames.json'\n",
    "\n",
    "if 'UD1_' in RUN: \n",
    "    print('yes')\n",
    "    X_filepath = '../02_output/{}/{}/{}_UD1/{}_{}.npz' # LANG, RUN, feat_class, dataset\n",
    "    y_filepath = '../02_output/{}/{}/{}_UD1/{}_{}.npy' # LANG, RUN, feat_class, dataset\n",
    "    featname_filepath = '../02_output/{}/{}/{}_UD1/{}_featnames.json'\n",
    "\n",
    "##### Loading the datasets\n",
    "for dataset in ['train', 'dev', 'test']: \n",
    "    for feat_class in feat_var_mapping: \n",
    "        print(feat_class, dataset)\n",
    "        var_name = 'featureset_'+feat_var_mapping[feat_class]+'_'+dataset\n",
    "        globals()['X_'+var_name] \\\n",
    "        = sparse.load_npz(X_filepath.format(LANG, RUN, feat_class, 'X', dataset))\n",
    "        globals()['y_'+var_name] \\\n",
    "        = np.load(y_filepath.format(LANG, RUN, feat_class, 'y', dataset))\n",
    "        # zero_based = True needed, the default auto setting does not detect the feature set properly\n",
    "        \n",
    "        try:    var_dict[dataset][feat_class] = ['X_'+var_name, 'y_'+var_name]\n",
    "        except: var_dict[dataset] = {feat_class: ['X_'+var_name, 'y_'+var_name]}\n",
    "        \n",
    "        with open(featname_filepath.format(LANG, RUN, feat_class,dataset), 'rb') as f:\n",
    "            globals()['featnames_'+ feat_var_mapping[feat_class]] = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dev': {'Li_etal16': ['X_featureset_Li16_dev', 'y_featureset_Li16_dev'],\n",
      "         'Lin_etal': ['X_featureset_Lin_dev', 'y_featureset_Lin_dev'],\n",
      "         'PitlerNenkova_Conn': ['X_featureset_PNconn_dev',\n",
      "                                'y_featureset_PNconn_dev']},\n",
      " 'test': {'Li_etal16': ['X_featureset_Li16_test', 'y_featureset_Li16_test'],\n",
      "          'Lin_etal': ['X_featureset_Lin_test', 'y_featureset_Lin_test'],\n",
      "          'PitlerNenkova_Conn': ['X_featureset_PNconn_test',\n",
      "                                 'y_featureset_PNconn_test']},\n",
      " 'train': {'Li_etal16': ['X_featureset_Li16_train', 'y_featureset_Li16_train'],\n",
      "           'Lin_etal': ['X_featureset_Lin_train', 'y_featureset_Lin_train'],\n",
      "           'PitlerNenkova_Conn': ['X_featureset_PNconn_train',\n",
      "                                  'y_featureset_PNconn_train']}}\n"
     ]
    }
   ],
   "source": [
    "pprint(var_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "PitlerNenkova_Conn train (49557, 101)\n",
      "Lin_etal train (49557, 31555)\n",
      "Li_etal16 train (49557, 31498)\n",
      "\n",
      "\n",
      "PitlerNenkova_Conn dev (2150, 101)\n",
      "Lin_etal dev (2150, 31555)\n",
      "Li_etal16 dev (2150, 31498)\n",
      "\n",
      "\n",
      "PitlerNenkova_Conn test (2897, 101)\n",
      "Lin_etal test (2897, 31555)\n",
      "Li_etal16 test (2897, 31498)\n"
     ]
    }
   ],
   "source": [
    "# check alignment in the number of features across datasets\n",
    "for dataset in var_dict:\n",
    "    print('\\n')\n",
    "    for featureset in var_dict[dataset]:\n",
    "        print(featureset, dataset, globals()[var_dict[dataset][featureset][0]].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Define the experiment featuresets and the experimental setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = {\n",
    "                1: ['PitlerNenkova_Conn'], \n",
    "                2: ['PitlerNenkova_Syn'], \n",
    "                3: ['Li_etal16'],\n",
    "                4: ['PitlerNenkova_Conn', 'PitlerNenkova_Syn'], \n",
    "                5: ['PitlerNenkova_Conn', 'PitlerNenkova_Syn', 'PitlerNenkova_ConnSyn'], \n",
    "                6: ['PitlerNenkova_Conn', 'PitlerNenkova_Syn', 'PitlerNenkova_ConnSyn', 'PitlerNenkova_SynSyn'], \n",
    "                7: ['PitlerNenkova_Conn', 'PitlerNenkova_Syn', 'PitlerNenkova_ConnSyn', 'PitlerNenkova_SynSyn', 'Lin_etal'],\n",
    "               }\n",
    "if 'Run2' in RUN:\n",
    "    experiments = {\n",
    "                1: ['PitlerNenkova_Conn'], \n",
    "                2: ['Li_etal16'], \n",
    "               } # PitlerNenkova_Conn to check performance change from PTB to UD. Li_etal16 is sota"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve the X, y train, dev data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_Xytraindev(experiment_spec):\n",
    "    features = experiment_spec\n",
    "    __train = [globals()[var_dict['train'][i][0]] for i in features]\n",
    "    X_train = sparse.hstack(__train)\n",
    "    y_train = [globals()[var_dict['train'][i][1]] for i in features][0]\n",
    "    \n",
    "    \n",
    "    __dev = [globals()[var_dict['dev'][i][0]] for i in features]\n",
    "    X_dev = sparse.hstack(__dev)\n",
    "    y_dev = [globals()[var_dict['dev'][i][1]] for i in features][0]\n",
    "    \n",
    "    return X_train, y_train, X_dev, y_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_featureset_PNconn_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_evalmetrics(actual, pred):\n",
    "    \"\"\"\n",
    "    Helper function to produce the binary classification metrics we require \n",
    "    \n",
    "    \"\"\"\n",
    "    acc = accuracy_score(actual, pred)\n",
    "    f1macro = f1_score(actual, pred, average='macro') \n",
    "    f1micro = f1_score(actual, pred, average='micro')\n",
    "    f1weight = f1_score(actual, pred, average='weighted')\n",
    "    \n",
    "    return acc, f1macro, f1micro, f1weight \n",
    "\n",
    "def _init_dimreducer(X_train, classifier, n_components=None, reduce_dim=None, random_state=42):\n",
    "    \"\"\"\n",
    "    Helper function to instantiate the dimensionality reduction method to be used. \n",
    "    Input | \n",
    "    Output| \n",
    "    \"\"\"\n",
    "    if reduce_dim == 'SparsePCA':\n",
    "        print('Starting dimension reduction with: {}'.format(reduce_dim))\n",
    "        dimreducer = IncrementalPCA(n_components=n_components)\n",
    "\n",
    "    if reduce_dim == 'TruncatedSVD':\n",
    "        print('Starting dimension reduction with: {}'.format(reduce_dim))\n",
    "        dimreducer = TruncatedSVD(n_components=n_components, random_state=random_state)\n",
    "\n",
    "    if reduce_dim == 'SRP' and X_train.shape[1]>10000:\n",
    "        print('Starting dimension reduction with: {}'.format(reduce_dim))\n",
    "        dimreducer = SparseRandomProjection(eps=0.1)\n",
    "\n",
    "    if reduce_dim == 'RFECV' and X_train.shape[1]>10000:\n",
    "        print('Starting dimension reduction with: {}'.format(reduce_dim))\n",
    "        dimreducer = RFECV(classifier, step = 0.2, min_features_to_select=n_components,\n",
    "                            cv = 3, n_jobs=-1, scoring='f1') \n",
    "                            # using f1 score instead of accuracy score \n",
    "        classifier = dimreducer\n",
    "        \n",
    "    return dimreducer\n",
    "\n",
    "def _init_classifier(classifier_name, class_weight = None, random_state = None):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    if classifier_name == 'logreg':\n",
    "        ##### LOGISTIC REGRESSION #####\n",
    "        # setting lower Cs (stronger regularisation) since we have very sparse matrices.  \n",
    "        # setting penalty to L1 as a form of feature selection, since we have so many features. \n",
    "        # l1 penalty usable only with liblinear or saga solver, but saga is faster \n",
    "        # fit_intercept = True since we have not normalised/standardised the featureset \n",
    "        classifier = LogisticRegressionCV(Cs = 10, fit_intercept=True, cv=10, dual=False, penalty='l2',\n",
    "                scoring=\"accuracy\", solver='saga', tol=0.0001, max_iter=100, class_weight=class_weight,\n",
    "                n_jobs=-1, verbose=0, refit=True, intercept_scaling=1.0, multi_class='warn', random_state=random_state) \n",
    "    if classifier_name == 'passaggressive':\n",
    "        ##### PASSIVE AGGRESSIVE CLASSIFIER #####\n",
    "        # setting C = 1.0 - high regularisation since we have sparsity in the data\n",
    "        # setting early_stopping=True and validation_fraction=0.2 to stop further training if not promising\n",
    "        # unsure about the distibution of the data at this point, so not setting tol\n",
    "        # C = 10, to match the max in logreg's Cs above. \n",
    "        classifier = PassiveAggressiveClassifier(C=10, fit_intercept=True, max_iter=None, tol=None,\n",
    "                early_stopping=True, validation_fraction=0.2, n_iter_no_change=5, shuffle=True,\n",
    "                verbose=0, loss='hinge', n_jobs=-1, random_state=random_state, warm_start=False,\n",
    "                class_weight=class_weight, average=False, n_iter=None)\n",
    "    \n",
    "    if classifier_name == 'linearsvc':\n",
    "        ##### LINEAR SVC ##### \n",
    "        # Using Linear SVC instead of SVM because it is faster \n",
    "        # and the task is a relatively 'simple' binary classification one\n",
    "        classifier = LinearSVC(penalty='l2', loss='squared_hinge', dual=True, tol=0.0001,\n",
    "        C=1.0, multi_class='ovr', fit_intercept=True, intercept_scaling=1,\n",
    "        class_weight=class_weight, verbose=0, random_state=random_state, max_iter=1000,)\n",
    "    \n",
    "    if classifier_name == 'sgdclass':\n",
    "        ##### SGD CLASSIFIER #####\n",
    "        classifier = SGDClassifier()\n",
    "    \n",
    "    if classifier_name == 'perceptron':\n",
    "        ##### PERCEPTRON #####\n",
    "        classifier = Perceptron()\n",
    "    return classifier\n",
    "\n",
    "# other notes: \n",
    "# 1. not including naive bayes because we have interaction features in the dataset. - possible to remove these \n",
    "#    features from the experimental set-up in order to try nb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(experiments, classifier, exp_id, shuffle_data = True, reduce_dim=None,\n",
    "                    n_components=500, random_state=42):\n",
    "    \"\"\"\n",
    "    Run the experiments defined, using an instance of a sklearn classifier. \n",
    "    \"\"\"\n",
    "    ### mlflow preliminaries \n",
    "    warnings.filterwarnings(\"ignore\") #bypass model training alerts/messages, for mlflow.\n",
    "    \n",
    "    exp_id = mlflow.set_experiment(exp_id)\n",
    "        \n",
    "    ### running experiments \n",
    "    for experiment_num in experiments:\n",
    "        print('FEATURESET COMPONENTS: ', experiments[experiment_num])\n",
    "        exp_components = [\"\".join(re.findall(r'[A-Z_0-9]+', i)) for i in experiments[experiment_num]]\n",
    "        # the experiment name is the cap letters, numbers and _ in featureset name\n",
    "        exp_name = \"_\".join(exp_components) # for run_name; in this experiment set-up, \n",
    "                                            # allows us to see the various feature components\n",
    "        \n",
    "        X_train, y_train, X_dev, y_dev = retrieve_Xytraindev(experiments[experiment_num])\n",
    "        \n",
    "        # shuffle the data  \n",
    "        if shuffle_data == True:\n",
    "            # shuffle the data \n",
    "            X_train, y_train = shuffle(X_train, y_train, random_state=random_state)\n",
    "            X_dev, y_dev = shuffle(X_dev, y_dev, random_state=random_state)\n",
    "            print('data shuffled', X_train.shape, X_dev.shape)\n",
    "        \n",
    "        # dimension reduction\n",
    "        if X_train.shape[1] > n_components:\n",
    "            _reduce_dim=reduce_dim \n",
    "            _n_components=n_components\n",
    "            \n",
    "            dimreducer = _init_dimreducer(X_train, classifier, n_components=n_components, \n",
    "                             reduce_dim=reduce_dim, random_state=random_state)\n",
    "            if reduce_dim == 'SparsePCA':\n",
    "                X_train = dimreducer.fit_transform(X_train.toarray())\n",
    "                X_dev = dimreducer.transform(X_dev.toarray())\n",
    "            elif reduce_dim == \"RFECV\":\n",
    "                pass\n",
    "            else: \n",
    "                X_train = dimreducer.fit_transform(X_train)\n",
    "                X_dev = dimreducer.transform(X_dev)     \n",
    "        else: \n",
    "            _reduce_dim=None\n",
    "            _n_components='NA'\n",
    "        \n",
    "        print('Dimension reduction with {}, new data shape'.format(_reduce_dim), \n",
    "              X_train.shape, X_dev.shape)\n",
    "\n",
    "        with mlflow.start_run(experiment_id=exp_id, run_name=exp_name, nested=True):\n",
    "            print('starting experiment')\n",
    "\n",
    "            # fit the classifier \n",
    "            classifier.fit(X_train, y_train)\n",
    "            print('classifier fitted')\n",
    "\n",
    "\n",
    "            # predict on train and text \n",
    "            y_train_pred = classifier.predict(X_train)\n",
    "            y_dev_pred = classifier.predict(X_dev)\n",
    "            \n",
    "            acc_train, f1macro_train, f1micro_train, f1weight_train = _get_evalmetrics(y_train, y_train_pred)\n",
    "            acc_dev, f1macro_dev, f1micro_dev, f1weight_dev = _get_evalmetrics(y_dev, y_dev_pred)\n",
    "\n",
    "            \n",
    "            ### Log parameter, metrics, and classifier to MLflow\n",
    "            # params: use sklearn's .get_params() to log all the parameters for the classifier \n",
    "            print([('classifier_'+param, classifier.get_params()[param]) for param in classifier.get_params()])\n",
    "            [mlflow.log_param('classifier_'+param, classifier.get_params()[param]) for param in classifier.get_params()]\n",
    "            mlflow.log_param('Classifier', classifier.__class__.__name__)\n",
    "            mlflow.log_param('Dimreducer', _reduce_dim)\n",
    "            # Names may only contain alphanumerics, underscores (_), dashes (-), periods (.), spaces ( ), and slashes (/).\n",
    "            try: [mlflow.log_param('dimreducer_'+param, dimreducer.get_params()[param]) \\\n",
    "                 for param in dimreducer.get_params()]\n",
    "            except: pass \n",
    "            \n",
    "            mlflow.log_metric(\"acc_train\", acc_train)\n",
    "            \n",
    "            # metrics\n",
    "            mlflow.log_metric(\"f1macro_train\", f1macro_train)\n",
    "            mlflow.log_metric(\"f1micro_train\", f1micro_train)\n",
    "            mlflow.log_metric(\"f1weight_train\", f1weight_train)\n",
    "            \n",
    "            mlflow.log_metric(\"acc_dev\", acc_dev)\n",
    "            mlflow.log_metric(\"f1macro_dev\", f1macro_dev)\n",
    "            mlflow.log_metric(\"f1micro_dev\", f1micro_dev)\n",
    "            mlflow.log_metric(\"f1weight_dev\", f1weight_dev)\n",
    "            \n",
    "            # classifier \n",
    "            mlflow.sklearn.log_model(classifier, \"model\")\n",
    "            print('model logged to mlflow')\n",
    "\n",
    "            # evaluate\n",
    "            print('Performance metrics...\\n')\n",
    "            print('train accuracy', acc_train)\n",
    "            print('dev accuracy', acc_dev,'\\n')\n",
    "            print('train f1macro', f1macro_train)\n",
    "            print('dev f1macro', f1macro_dev,'\\n')\n",
    "            print('train f1micro', f1micro_train)\n",
    "            print('dev f1micro', f1micro_dev,'\\n')\n",
    "            print('train f1weighted', f1weight_train)\n",
    "            print('dev f1weighted', f1weight_dev,'\\n')\n",
    "            print('__________\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Running the experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PTB featureset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEATURESET COMPONENTS:  ['PitlerNenkova_Conn']\n",
      "data shuffled (49557, 101) (2150, 101)\n",
      "Dimension reduction with None, new data shape (49557, 101) (2150, 101)\n",
      "starting experiment\n",
      "classifier fitted\n",
      "[('classifier_Cs', 10), ('classifier_class_weight', {0: 0.7021400100615302, 1: 0.29785998993846985}), ('classifier_cv', 10), ('classifier_dual', False), ('classifier_fit_intercept', True), ('classifier_intercept_scaling', 1.0), ('classifier_max_iter', 100), ('classifier_multi_class', 'warn'), ('classifier_n_jobs', -1), ('classifier_penalty', 'l2'), ('classifier_random_state', 42), ('classifier_refit', True), ('classifier_scoring', 'accuracy'), ('classifier_solver', 'saga'), ('classifier_tol', 0.0001), ('classifier_verbose', 0)]\n",
      "model logged to mlflow\n",
      "Performance metrics...\n",
      "\n",
      "train accuracy 0.8435538874427427\n",
      "dev accuracy 0.8251162790697675 \n",
      "\n",
      "train f1macro 0.7880027996080748\n",
      "dev f1macro 0.7672541168151703 \n",
      "\n",
      "train f1micro 0.8435538874427427\n",
      "dev f1micro 0.8251162790697675 \n",
      "\n",
      "train f1weighted 0.8320464384293725\n",
      "dev f1weighted 0.8098950759393604 \n",
      "\n",
      "__________\n",
      "\n",
      "FEATURESET COMPONENTS:  ['Li_etal16']\n",
      "data shuffled (49557, 32683) (2150, 32683)\n",
      "Starting dimension reduction with: SRP\n",
      "Dimension reduction with SRP, new data shape (49557, 9266) (2150, 9266)\n",
      "starting experiment\n",
      "classifier fitted\n",
      "[('classifier_Cs', 10), ('classifier_class_weight', {0: 0.7021400100615302, 1: 0.29785998993846985}), ('classifier_cv', 10), ('classifier_dual', False), ('classifier_fit_intercept', True), ('classifier_intercept_scaling', 1.0), ('classifier_max_iter', 100), ('classifier_multi_class', 'warn'), ('classifier_n_jobs', -1), ('classifier_penalty', 'l2'), ('classifier_random_state', 42), ('classifier_refit', True), ('classifier_scoring', 'accuracy'), ('classifier_solver', 'saga'), ('classifier_tol', 0.0001), ('classifier_verbose', 0)]\n",
      "model logged to mlflow\n",
      "Performance metrics...\n",
      "\n",
      "train accuracy 0.9540125512036645\n",
      "dev accuracy 0.9065116279069767 \n",
      "\n",
      "train f1macro 0.9430133934219147\n",
      "dev f1macro 0.8842429828586398 \n",
      "\n",
      "train f1micro 0.9540125512036645\n",
      "dev f1micro 0.9065116279069767 \n",
      "\n",
      "train f1weighted 0.9531744287179065\n",
      "dev f1weighted 0.9028985815735774 \n",
      "\n",
      "__________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### Environment variables \n",
    "CLASS_WEIGHT={0: 0.7021400100615302, 1: 0.29785998993846985}\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# note: setting high Cs for L1 regularisation is akin to dimensionality reduction, \n",
    "# so we need to be mindful of settings for reduce_dim\n",
    "\n",
    "for classifier_name in ['logreg']:\n",
    "    # instantiate the classifer \n",
    "    classifier = _init_classifier(classifier_name, class_weight = CLASS_WEIGHT, \n",
    "                                  random_state = RANDOM_STATE)\n",
    "    \n",
    "    # run the experimental pipeline\n",
    "    run_experiments(experiments, classifier, exp_id = RUN+'_'+LANG, \n",
    "                    shuffle_data = True, reduce_dim='SRP',\n",
    "                    n_components=4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UD featureset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEATURESET COMPONENTS:  ['PitlerNenkova_Conn']\n",
      "data shuffled (49557, 101) (2150, 101)\n",
      "Dimension reduction with None, new data shape (49557, 101) (2150, 101)\n",
      "starting experiment\n",
      "classifier fitted\n",
      "[('classifier_Cs', 10), ('classifier_class_weight', {0: 0.7021400100615302, 1: 0.29785998993846985}), ('classifier_cv', 10), ('classifier_dual', False), ('classifier_fit_intercept', True), ('classifier_intercept_scaling', 1.0), ('classifier_max_iter', 100), ('classifier_multi_class', 'warn'), ('classifier_n_jobs', -1), ('classifier_penalty', 'l2'), ('classifier_random_state', 42), ('classifier_refit', True), ('classifier_scoring', 'accuracy'), ('classifier_solver', 'saga'), ('classifier_tol', 0.0001), ('classifier_verbose', 0)]\n",
      "model logged to mlflow\n",
      "Performance metrics...\n",
      "\n",
      "train accuracy 0.8435538874427427\n",
      "dev accuracy 0.8251162790697675 \n",
      "\n",
      "train f1macro 0.7880027996080748\n",
      "dev f1macro 0.7672541168151703 \n",
      "\n",
      "train f1micro 0.8435538874427427\n",
      "dev f1micro 0.8251162790697675 \n",
      "\n",
      "train f1weighted 0.8320464384293725\n",
      "dev f1weighted 0.8098950759393604 \n",
      "\n",
      "__________\n",
      "\n",
      "FEATURESET COMPONENTS:  ['Li_etal16']\n",
      "data shuffled (49557, 31498) (2150, 31498)\n",
      "Starting dimension reduction with: SRP\n",
      "Dimension reduction with SRP, new data shape (49557, 9266) (2150, 9266)\n",
      "starting experiment\n",
      "classifier fitted\n",
      "[('classifier_Cs', 10), ('classifier_class_weight', {0: 0.7021400100615302, 1: 0.29785998993846985}), ('classifier_cv', 10), ('classifier_dual', False), ('classifier_fit_intercept', True), ('classifier_intercept_scaling', 1.0), ('classifier_max_iter', 100), ('classifier_multi_class', 'warn'), ('classifier_n_jobs', -1), ('classifier_penalty', 'l2'), ('classifier_random_state', 42), ('classifier_refit', True), ('classifier_scoring', 'accuracy'), ('classifier_solver', 'saga'), ('classifier_tol', 0.0001), ('classifier_verbose', 0)]\n",
      "model logged to mlflow\n",
      "Performance metrics...\n",
      "\n",
      "train accuracy 0.9673305486611377\n",
      "dev accuracy 0.9325581395348838 \n",
      "\n",
      "train f1macro 0.9600867609993107\n",
      "dev f1macro 0.9191703127532005 \n",
      "\n",
      "train f1micro 0.9673305486611377\n",
      "dev f1micro 0.9325581395348838 \n",
      "\n",
      "train f1weighted 0.9669877794363287\n",
      "dev f1weighted 0.9312576077903774 \n",
      "\n",
      "__________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### Environment variables \n",
    "CLASS_WEIGHT={0: 0.7021400100615302, 1: 0.29785998993846985}\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# note: setting high Cs for L1 regularisation is akin to dimensionality reduction, \n",
    "# so we need to be mindful of settings for reduce_dim\n",
    "\n",
    "for classifier_name in ['logreg']:\n",
    "    # instantiate the classifer \n",
    "    classifier = _init_classifier(classifier_name, class_weight = CLASS_WEIGHT, \n",
    "                                  random_state = RANDOM_STATE)\n",
    "    \n",
    "    # run the experimental pipeline\n",
    "    run_experiments(experiments, classifier, exp_id = RUN+'_'+LANG, \n",
    "                    shuffle_data = True, reduce_dim='SRP',\n",
    "                    n_components=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
